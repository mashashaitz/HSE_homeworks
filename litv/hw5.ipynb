{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('imdb_master.txt', sep=',', engine='python')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/masha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>unsup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>mr costner dragged movie far longer necessary ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[mr, costner, dragged, movie, far, longer, nec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>example majority action film  generic boring  ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[example, majority, action, film, , generic, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>first hate moronic rapper  couldnt act gun pre...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[first, hate, moronic, rapper, , couldnt, act,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>even beatles could write song everyone liked  ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[even, beatles, could, write, song, everyone, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>brass picture  movie fitting word  really some...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[brass, picture, , movie, fitting, word, , rea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review label  \\\n",
       "0  test  mr costner dragged movie far longer necessary ...   neg   \n",
       "1  test  example majority action film  generic boring  ...   neg   \n",
       "2  test  first hate moronic rapper  couldnt act gun pre...   neg   \n",
       "3  test  even beatles could write song everyone liked  ...   neg   \n",
       "4  test  brass picture  movie fitting word  really some...   neg   \n",
       "\n",
       "                                              tokens  neg  pos  unsup  \n",
       "0  [mr, costner, dragged, movie, far, longer, nec...    1    0      0  \n",
       "1  [example, majority, action, film, , generic, b...    1    0      0  \n",
       "2  [first, hate, moronic, rapper, , couldnt, act,...    1    0      0  \n",
       "3  [even, beatles, could, write, song, everyone, ...    1    0      0  \n",
       "4  [brass, picture, , movie, fitting, word, , rea...    1    0      0  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "toks = []\n",
    "reviews = []\n",
    "for r in df['review']:\n",
    "    tokens = nltk.word_tokenize(r)\n",
    "    l = []\n",
    "    for t in tokens:\n",
    "        tnew = t.lower()\n",
    "        tnew = re.sub(r'[^\\w\\s]','',tnew)\n",
    "        if tnew not in stoplist:\n",
    "            l.append(lemmatizer.lemmatize(tnew))\n",
    "    toks.append(l)\n",
    "    reviews.append(' '.join(l))\n",
    "df['review'] = reviews\n",
    "df['tokens'] = toks\n",
    "\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0', 'file'])\n",
    "\n",
    "\n",
    "negs = []\n",
    "poss = []\n",
    "unsup = []\n",
    "ls = []\n",
    "for l in df['label']:\n",
    "    if l == 'neg':\n",
    "        negs.append(1)\n",
    "        poss.append(0)\n",
    "        unsup.append(0)\n",
    "    elif l == 'pos':\n",
    "        negs.append(0)\n",
    "        poss.append(1)\n",
    "        unsup.append(0)\n",
    "    elif l == 'unsup':\n",
    "        negs.append(0)\n",
    "        poss.append(0)\n",
    "        unsup.append(1)\n",
    "df['neg'] = negs \n",
    "df['pos'] = poss \n",
    "df['unsup'] = unsup \n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.type == 'train']\n",
    "df_test =  df[df.type == 'test']\n",
    "df_train = df_train[['review', 'pos', 'tokens', 'neg', 'unsup']]\n",
    "df_test = df_test[['review', 'pos', 'tokens', 'neg', 'unsup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>pos</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg</th>\n",
       "      <th>unsup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story man unnatural feeling pig  start opening...</td>\n",
       "      <td>0</td>\n",
       "      <td>[story, man, unnatural, feeling, pig, , start,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport 77 start brand new luxury 747 plane lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[airport, 77, start, brand, new, luxury, 747, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>film lacked something could nt put finger firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>[film, lacked, something, could, nt, put, fing...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone    know supposed  art  film   w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[sorry, everyone, , , , know, supposed, , art,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>little parent took along theater see interior ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[little, parent, took, along, theater, see, in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  pos  \\\n",
       "25000  story man unnatural feeling pig  start opening...    0   \n",
       "25001  airport 77 start brand new luxury 747 plane lo...    0   \n",
       "25002  film lacked something could nt put finger firs...    0   \n",
       "25003  sorry everyone    know supposed  art  film   w...    0   \n",
       "25004  little parent took along theater see interior ...    0   \n",
       "\n",
       "                                                  tokens  neg  unsup  \n",
       "25000  [story, man, unnatural, feeling, pig, , start,...    1      0  \n",
       "25001  [airport, 77, start, brand, new, luxury, 747, ...    1      0  \n",
       "25002  [film, lacked, something, could, nt, put, fing...    1      0  \n",
       "25003  [sorry, everyone, , , , know, supposed, , art,...    1      0  \n",
       "25004  [little, parent, took, along, theater, see, in...    1      0  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>pos</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg</th>\n",
       "      <th>unsup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mr costner dragged movie far longer necessary ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mr, costner, dragged, movie, far, longer, nec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>example majority action film  generic boring  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[example, majority, action, film, , generic, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first hate moronic rapper  couldnt act gun pre...</td>\n",
       "      <td>0</td>\n",
       "      <td>[first, hate, moronic, rapper, , couldnt, act,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even beatles could write song everyone liked  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[even, beatles, could, write, song, everyone, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brass picture  movie fitting word  really some...</td>\n",
       "      <td>0</td>\n",
       "      <td>[brass, picture, , movie, fitting, word, , rea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  pos  \\\n",
       "0  mr costner dragged movie far longer necessary ...    0   \n",
       "1  example majority action film  generic boring  ...    0   \n",
       "2  first hate moronic rapper  couldnt act gun pre...    0   \n",
       "3  even beatles could write song everyone liked  ...    0   \n",
       "4  brass picture  movie fitting word  really some...    0   \n",
       "\n",
       "                                              tokens  neg  unsup  \n",
       "0  [mr, costner, dragged, movie, far, longer, nec...    1      0  \n",
       "1  [example, majority, action, film, , generic, b...    1      0  \n",
       "2  [first, hate, moronic, rapper, , couldnt, act,...    1      0  \n",
       "3  [even, beatles, could, write, song, everyone, ...    1      0  \n",
       "4  [brass, picture, , movie, fitting, word, , rea...    1      0  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12598050 words total, with a vocabulary size of 166195\n",
      "Max sentence length is 1923 words\n"
     ]
    }
   ],
   "source": [
    "all_training_words = []\n",
    "training_sentence_lengths = []\n",
    "for s in df_train['tokens']:\n",
    "    training_sentence_lengths.append(len(s))\n",
    "    for t in s:\n",
    "        all_training_words.append(t)\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s words\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096662 words total, with a vocabulary size of 86080\n",
      "Max sentence length is 1713 words\n"
     ]
    }
   ],
   "source": [
    "all_test_words = []\n",
    "test_sentence_lengths = []\n",
    "for s in df_test['tokens']:\n",
    "    test_sentence_lengths.append(len(s))\n",
    "    for t in s:\n",
    "        all_test_words.append(t)\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print('%s words total, with a vocabulary size of %s' % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print('Max sentence length is %s words' % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-31 00:54:06--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "R'esolution de s3.amazonaws.com (s3.amazonaws.com)... 52.216.171.117\n",
      "Connexion `a s3.amazonaws.com (s3.amazonaws.com)|52.216.171.117|:443... connect'e.\n",
      "requ^ete HTTP transmise, en attente de la r'eponse... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    Le fichier a d'ej`a 'et'e compl`etement r'ecup'er'e ; rien `a faire.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165885 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(df_train['review'].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(df_train['review'].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(df_test['review'].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165886, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "train_embeddings = get_word2vec_embeddings(word2vec, df_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_embeddings = []\\nfor t in df_train['tokens'].tolist():\\n    vec = []\\n    for w in t:\\n        if w in word2vec:\\n            vec.append(word2vec[w])\\n        else: \\n            vec.append(np.random.rand(300))\\n        \\n    train_embeddings.append(np.divide(np.sum(vec, axis=0), len(vec)))\\n\""
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_embeddings = []\n",
    "for t in df_train['tokens'].tolist():\n",
    "    vec = []\n",
    "    for w in t:\n",
    "        if w in word2vec:\n",
    "            vec.append(word2vec[w])\n",
    "        else: \n",
    "            vec.append(np.random.rand(300))\n",
    "        \n",
    "    train_embeddings.append(np.divide(np.sum(vec, axis=0), len(vec)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['pos', 'neg', 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 50, 300)      49765800    input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 49, 200)      120200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 48, 200)      180200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 47, 200)      240200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 46, 200)      300200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 45, 200)      360200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_50 (Global (None, 200)          0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_51 (Global (None, 200)          0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_52 (Global (None, 200)          0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_53 (Global (None, 200)          0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_54 (Global (None, 200)          0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1000)         0           global_max_pooling1d_50[0][0]    \n",
      "                                                                 global_max_pooling1d_51[0][0]    \n",
      "                                                                 global_max_pooling1d_52[0][0]    \n",
      "                                                                 global_max_pooling1d_53[0][0]    \n",
      "                                                                 global_max_pooling1d_54[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1000)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          128128      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 128)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 3)            387         dropout_15[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 51,095,315\n",
      "Trainable params: 1,329,515\n",
      "Non-trainable params: 49,765,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Input, Conv1D, GlobalMaxPooling1D, concatenate, Dropout, Dense\n",
    "from keras.models import Model\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67500 samples, validate on 7500 samples\n",
      "Epoch 1/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.5153 - acc: 0.7517 - val_loss: 0.2753 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.4915 - acc: 0.7525 - val_loss: 0.3311 - val_acc: 0.9998\n",
      "Epoch 3/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.4733 - acc: 0.7516 - val_loss: 0.2793 - val_acc: 0.9967\n",
      "Epoch 4/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.4391 - acc: 0.7598 - val_loss: 0.2931 - val_acc: 0.9776\n",
      "Epoch 5/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.3761 - acc: 0.8073 - val_loss: 0.2412 - val_acc: 0.9194\n",
      "Epoch 6/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.2967 - acc: 0.8627 - val_loss: 0.4861 - val_acc: 0.7736\n",
      "Epoch 7/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.2316 - acc: 0.9017 - val_loss: 0.2497 - val_acc: 0.8960\n",
      "Epoch 8/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.1893 - acc: 0.9249 - val_loss: 0.5917 - val_acc: 0.7866\n",
      "Epoch 9/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.1614 - acc: 0.9383 - val_loss: 0.2930 - val_acc: 0.8856\n",
      "Epoch 10/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.1414 - acc: 0.9472 - val_loss: 0.4573 - val_acc: 0.8263\n",
      "Epoch 11/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.1264 - acc: 0.9540 - val_loss: 0.2643 - val_acc: 0.9076\n",
      "Epoch 12/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.1114 - acc: 0.9592 - val_loss: 0.3099 - val_acc: 0.8945\n",
      "Epoch 13/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.1012 - acc: 0.9643 - val_loss: 0.5700 - val_acc: 0.8200\n",
      "Epoch 14/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.0939 - acc: 0.9672 - val_loss: 0.3830 - val_acc: 0.8864\n",
      "Epoch 15/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.0878 - acc: 0.9702 - val_loss: 0.5443 - val_acc: 0.8466\n",
      "Epoch 16/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.0804 - acc: 0.9724 - val_loss: 0.6925 - val_acc: 0.8018\n",
      "Epoch 17/50\n",
      "67500/67500 [==============================] - 230s 3ms/step - loss: 0.0743 - acc: 0.9737 - val_loss: 0.5914 - val_acc: 0.8302\n",
      "Epoch 18/50\n",
      "67500/67500 [==============================] - 228s 3ms/step - loss: 0.0716 - acc: 0.9756 - val_loss: 0.7320 - val_acc: 0.7857\n",
      "Epoch 19/50\n",
      "67500/67500 [==============================] - 229s 3ms/step - loss: 0.0649 - acc: 0.9778 - val_loss: 0.7141 - val_acc: 0.7980\n",
      "Epoch 20/50\n",
      "67500/67500 [==============================] - 229s 3ms/step - loss: 0.0590 - acc: 0.9796 - val_loss: 0.7235 - val_acc: 0.8162\n",
      "Epoch 21/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.0596 - acc: 0.9795 - val_loss: 0.9002 - val_acc: 0.7658\n",
      "Epoch 22/50\n",
      "67500/67500 [==============================] - 231s 3ms/step - loss: 0.0535 - acc: 0.9816 - val_loss: 1.1607 - val_acc: 0.7583\n",
      "Epoch 23/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.0518 - acc: 0.9821 - val_loss: 0.3787 - val_acc: 0.8976\n",
      "Epoch 24/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.0511 - acc: 0.9823 - val_loss: 0.5566 - val_acc: 0.8638\n",
      "Epoch 25/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.0482 - acc: 0.9830 - val_loss: 0.5223 - val_acc: 0.8672\n",
      "Epoch 26/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.0487 - acc: 0.9834 - val_loss: 0.3407 - val_acc: 0.9104\n",
      "Epoch 27/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0479 - acc: 0.9835 - val_loss: 0.6643 - val_acc: 0.8402\n",
      "Epoch 28/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0435 - acc: 0.9846 - val_loss: 0.8116 - val_acc: 0.8180\n",
      "Epoch 29/50\n",
      "67500/67500 [==============================] - 232s 3ms/step - loss: 0.0440 - acc: 0.9850 - val_loss: 0.3334 - val_acc: 0.9156\n",
      "Epoch 30/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0417 - acc: 0.9852 - val_loss: 0.5983 - val_acc: 0.8541\n",
      "Epoch 31/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0407 - acc: 0.9856 - val_loss: 0.8815 - val_acc: 0.8076\n",
      "Epoch 32/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0406 - acc: 0.9856 - val_loss: 0.7443 - val_acc: 0.8354\n",
      "Epoch 33/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0390 - acc: 0.9865 - val_loss: 0.5770 - val_acc: 0.8752\n",
      "Epoch 34/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0391 - acc: 0.9860 - val_loss: 0.7009 - val_acc: 0.8491\n",
      "Epoch 35/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0355 - acc: 0.9874 - val_loss: 0.4265 - val_acc: 0.9027\n",
      "Epoch 36/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0354 - acc: 0.9875 - val_loss: 0.4108 - val_acc: 0.8981\n",
      "Epoch 37/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0373 - acc: 0.9873 - val_loss: 1.0032 - val_acc: 0.8318\n",
      "Epoch 38/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0366 - acc: 0.9873 - val_loss: 1.0248 - val_acc: 0.7995\n",
      "Epoch 39/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0357 - acc: 0.9875 - val_loss: 0.9462 - val_acc: 0.8027\n",
      "Epoch 40/50\n",
      "67500/67500 [==============================] - 233s 3ms/step - loss: 0.0344 - acc: 0.9881 - val_loss: 0.7785 - val_acc: 0.8376\n",
      "Epoch 41/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0339 - acc: 0.9881 - val_loss: 0.7256 - val_acc: 0.8512\n",
      "Epoch 42/50\n",
      "67500/67500 [==============================] - 235s 3ms/step - loss: 0.0323 - acc: 0.9888 - val_loss: 0.8704 - val_acc: 0.8348\n",
      "Epoch 43/50\n",
      "67500/67500 [==============================] - 235s 3ms/step - loss: 0.0326 - acc: 0.9883 - val_loss: 0.6364 - val_acc: 0.8723\n",
      "Epoch 44/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0320 - acc: 0.9891 - val_loss: 0.8325 - val_acc: 0.8395\n",
      "Epoch 45/50\n",
      "67500/67500 [==============================] - 235s 3ms/step - loss: 0.0326 - acc: 0.9887 - val_loss: 0.8494 - val_acc: 0.8220\n",
      "Epoch 46/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0308 - acc: 0.9892 - val_loss: 0.7412 - val_acc: 0.8524\n",
      "Epoch 47/50\n",
      "67500/67500 [==============================] - 235s 3ms/step - loss: 0.0306 - acc: 0.9900 - val_loss: 0.9151 - val_acc: 0.8277\n",
      "Epoch 48/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0301 - acc: 0.9895 - val_loss: 1.0820 - val_acc: 0.8064\n",
      "Epoch 49/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0311 - acc: 0.9893 - val_loss: 0.6621 - val_acc: 0.8699\n",
      "Epoch 50/50\n",
      "67500/67500 [==============================] - 234s 3ms/step - loss: 0.0307 - acc: 0.9897 - val_loss: 0.4926 - val_acc: 0.9023\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 19s 777us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [2, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43936"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test.pos==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10984"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test.pos==prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: pos, dtype: int64"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
