{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special thanks to Live Journal and its tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = '''Оксана Сбоку припёка.\n",
    "В Оксане Лавреньевой напрочь отсутствует порода. Помните мультфильм про собачку Соню, которая наелась горчицы? Вот Оксана чем-то сильно напоминает мне этот персонаж.\n",
    "Отсутствие достоинства -- а как ещё можно было лежать под жирным, потным боровом-банкиром, а потом стать «мамочкой» писаке-голодранцу? -- это первое, что бросается в глаза при взгляде на эту женщину.\n",
    "На любых групповых фотографиях Лаврентьева -- сбоку припёка. Отсутсвие стержня уверенности в себе она пытается скрыть за маской хохотушки.\n",
    "Есть в этом, на мой взгляд, что-то жалкое, что-то ссыкливое и ищущее протекции, что-то жмущееся к ноге сильного. Пусть даже эта нога вовсе и несильная, пусть и в любой момент пнуть может.\n",
    "Единственный человек во всём нашем светском курятнике, рядом с которым беспородность Оксаны не так сильно бросается в глаза, -- это Яна Рудковская.\n",
    "Почему?\n",
    "А Яна -- точно такая же: в ней, как и в Оксане, есть что-то от половой тряпки, несмотря на броню из лейблов, понты и морду кирпичом «У меня всё -- ок».\n",
    "Не ок. Никогда не было ок и не будет, потому что достоинства нет.\n",
    "А как у вас обстоит дело с достоинством? Теряли ли его? Ради чего?\n",
    "Рассказывайте!\n",
    "Я вас слушаю очень внимательно. Помогать буду.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws1 = ['Достоинство', 'Обсудить']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = '''Обычный выходной в России.\n",
    "Улыбнул ролик на сербском канале под названием \"Обычный выходной в России\".\n",
    "Видимо с каких-то показательных стрельб из РСЗО.\n",
    "Касательно РСЗО, то:\n",
    "1. В армию РФ полностью отгружены все заказанные комплексы \"Торнадо-Г\". Также заявлено, что все намеченные работы по модернизации \"Градов\" до уровня \"Торнадо-Г\" выполнены в срок.\n",
    "2. На Украине на вооружение принята РСЗО \"Верба\", представляющая из себя несколько видоизмененный \"Град\", причем как отмечают специалисты, не в лучшую сторону.\n",
    "3. Также на Украине переданы в войска 100 ракет для РСЗО \"Ольха\".(еще одна вариация на тему РСЗО на базе \"Смерча\") - в пакете 12 ракет с заявленной дальностью от 70 до 120 км.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws2 = ['РСЗО', 'Россия', 'Украина']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = '''Фото: Комитет по развитию туризма Санкт-Петербурга.\n",
    "Звонят мне тут на днях журналисты и спрашивают, что я думаю относительно нового логотипа Санкт-Петербурга. Я, конечно, удивился, что по такой важной теме хотят услышать комментарий человека, который не имеет никакого отношения ни к Петербургу, ни к дизайну логотипов, но своим мнением с журналисткой решил поделиться.\n",
    "— Илья, какие эмоции у вас вызывает новый логотип?\n",
    "— Знаете, единственная эмоцию, которую он у меня вызывает – это «Нахуя?». Был ведь отличный логотип, который сделала «Студия Лебедева», зачем было заказывать новый за семь миллионов, я так и не понял.\n",
    "— Ну а он вам вообще нравится?\n",
    "— Мне старый нравился больше, а этот... не знаю, нормальный.\n",
    "— О, спасибо!\n",
    "На следующее утро открываю газету и с удивлением узнаю, что я, оказывается, поддержал новый логотип!\n",
    "Вот так бывает, когда в бюджет на дизайн закладывают продвижение в СМИ. \n",
    "Кстати, отличный логотип, который в своё время сделала «Студия Лебедева» за символический 1 рубль, можно использовать совершенно свободно.\n",
    "Источник: «Студия Артемия Лебедева».\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws3 = ['Дизайн', 'Лебедев', 'Логотип', 'Санкт-Петербург']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = '''Нулевая смертность на дорогах и как её достичь.\n",
    "Источник: «Городские проекты».\n",
    "17 ноября — Всемирный день день памяти жертв ДТП. Сегодня «Городские проекты» напоминают о своих текстах, посвящённых программе Vision Zero, призванной снизить смертность на дорогах. В её основе лежат снижение скорости и правильное проектирование улиц, направленное на успокоение траффика и хорошую видимость.\n",
    "Гибель на дороге — абсолютно ненормальное явление. Для человека нет ничего естественного в том, чтобы выйти из дома утром и не вернуться вечером, потому что его сбила машина. Давайте сегодня освежим нашу память о Vision Zero и посмотрим, к каким результатам привело внедрение программы в городах мира и как ещё можно добиться снижения числа смертей на дорогах.\n",
    "Vision Zero — шведская программа, там она была принята парламентом ещё в 1997 году. За прошедшие годы смертность на шведских дорогах упала в два раза, при том, что автомобилизация только возросла.\n",
    "«Городские проекты» приводят фрагменты из интервью одного из идеологов программы, Матса-Оке Белина, занимающего должность стратега по транспортной безопасности Шведского управления по транспорту. Кроме прочего, он поясняет, что большая часть ДТП вызвана человеческим фактором и не соглашается с необходимостью полностью искоренить автомобильный трафик ради нулевой смертности на дорогах. \n",
    "Напротив — в его видении для снижения смертности необходимо снижать скорость автомобилей в жилых районах до 30 км/ч и менять классические перекрёстки на круговые. Дорожные камеры должны не пугать водителей возможным штрафом за превышение, а предупреждать о возможной аварии. Такую политику он называет «мягким принуждением».\n",
    "Так что это не война между незащищёнными пешеходами и защищёнными автомобилистами. Здесь нужен комплексный подход. Когда автомобили нужны, потому что они приносят пользу обществу, мы должны их использовать. Но там, где они нам не нужны, мы должны по максимуму отказаться от них.\n",
    "Источник: «Городские проекты».\n",
    "По мнению Белина, программу Vision Zero неверно понимают в Нью-Йорке. Однако, глядя на статистику, так не скажешь. В 2017 году на дорогах Нью-Йорка погибли 222 человека (почти половина из них — пешеходы), и это минимум с 1910 года! В 2013 году, накануне внедрения Vision Zero смертность составила 299 человек. Мэр де Блазио считает, что снизить число жертв среди пешеходов помогут создание удобной для пешеходов инфраструктуры и ужесточение законов для автомобилистов.\n",
    "И Нью-Йорк не совершенен. Среди его проблем незащищённые велодорожки и бюрократические проволочки, которые прошлым летом едва не привели к отключению камер перед 140 школами.\n",
    "Пик смертности на нью-йоркских дорогах был зафиксирован в 1929 году — 1360 погибших. И это при том, что в те дни городу было далеко до сегодняшней загруженности автомобилями. Но достаточно посмотреть статистику относительно недалекого 1990 года — 701 смерть — чтобы оценить сегодняшний прогресс.\n",
    "Источник: «Городские проекты».\n",
    "Следующий пример неожиданный — Богота, столица Колумбии, смертность на дорогах которой за 10 лет уменьшилась более чем на 60 процентов. Как и другие подобные города, Богота страдает от социального неравенства, а в связи с этим растёт и смертность на дорогах в бедных кварталах.\n",
    "Решением проблемы стала городская программа «Жизнь священна», проводившая свои идеи через мимов, высмеивающих нарушителей правил и поддерживающих законопослушных граждан, а также городской полиции, взявшей на себя обязанности своих упразднённой дорожной полиции — последним общество не доверяло.\n",
    "Одновременно в городе развивалось велодвижение и развивалась система скоростных автобусов. Улучшение транспортной инфраструктуры снизило аварийность на дорогах, передвижение по городу стало более безопасным, а использование частных автомобилей снизилось.\n",
    "При этом описанные перемены происходили в 1996-2006 годы, а программа Vision Zero действует в Боготе только с 2017 года!\n",
    "В колумбийской столице живет 8 миллионов человек, которые за сутки совершают порядка 15 миллионов перемещений по всему городу. Большая часть ходит пешком или пользуется автобусами. И вот что интересно — в результате на 15 миллионов приходится лишь 190 тысяч автомобильных поездок.\n",
    "Источник: «Городские проекты».\n",
    "Пример из Европы — Брюссель. В этом году я уже рассказывал вам о центре Брюсселя, в котором у машин остаётся всё меньше привилегий перед пешеходами. И вот недавно стало известно, что ограничение скорости до 30 км/ч будет действовать не только в пределах Малого кольца, но и во всём Столичном регионе, сопоставимом по масштабам с Белгородом и превышающим Москву в пределах ТТК!\n",
    "Вместе с новым ограничением скорости город намерен запретить дизельные автомобили и развивать общественный транспорт. Исключение в виде ограничения до 50 и 70 км/ч будет действовать лишь на нескольких оживлённых улицах.\n",
    "«Водители в Брюсселе будут знать, что, если не указано иное, ограничение скорости — 30 км/ч. Этот проход противоположен нынешнему, когда 30 км/ч — исключение», — добавила ван ден Брандт. Кроме этого, будут прилагаться усилия, чтобы исправить ситуацию в тридцати самых опасных местах Брюсселя, где происходит больше всего аварий.\n",
    "Этим летом в одном из брюссельских районов автомобилист травмировал 14-летнюю девочку и скрылся с места происшествия. Вслед за этим жители вышли на лежачий протест.\n",
    "Источник: «Городские проекты».\n",
    "Последний зарубежный пример, на этот раз из Лондона. Здесь, как и в упоминавшейся Боготе, одним из факторов смертельных ДТП стал социальный фактор. Проанализировав количество столкновений, повлекших за собой травму или смерть пешехода, специалисты обнаружили, что самым опасным является район Баркинг и Дагенем, где 825 пеших прогулок из миллиарда заканчиваются травмой или гибелью пешехода при среднегородском значении в 600 случаев на миллиард. Противоположный пример — район Кингстон-апон-Темс — 365 случаев на миллиард. \n",
    "При этом пешеходов в Баркинг и Дагенеме не так много, и этот район значительно беднее Кингстон-апон-Темс, что возвращает нас к разрыву между богатыми и бедными в безопасности на дорогах.\n",
    "Сейчас в Лондоне вводится программа Vision Zero, в рамках которой мэр планирует ввести ограничение скорости до 20 миль в час в зонах с платным въездом, что вместе с другими мерами должно привести к значительному снижению смертности на дорогах.\n",
    "«Мы не знаем, почему существует такой разрыв, хотя он коррелирует с другими доказательствами того, что пешеходы с низким заработком больше подвержены риску получить травмы, чем более обеспеченные», сообщила Элдред. «Это подтверждает необходимость принимать меры по снижению опасности на дорогах по всему Лондону — понижать разрешённую скорость и обеспечивать соблюдение скоростного режима».\n",
    "Источник: «Городские проекты».\n",
    "А что в России? К сожалению, вопрос снижения смертности на дорогах не поднимается на том же уровне, что и в приведённых выше примерах, оставаясь на устах активистов. Недавно мундеп Анастасия Ромашкевич разобрала одно из множества московских ДТП с точки зрения безопасности пешеходов на данном участке и опубликовала его результаты на «Городских проектах».\n",
    "5 ноября на пересечении Шмитовского проезда и 2-й Звенигородской улицы под колёса автомобиля попали два школьника, девяти и одиннадцати лет, переходившие дорогу по нерегулируемому пешеходному переходу. Изначально журналисты сообщили, что дети переходили дорогу в неположенном месте и умолчали о том, что поблизости находится школа. Это далеко не первое ДТП в этом месте, и после смертельного столкновения в 2016 году «Городские проекты» направили в дептанс предложения по редизайну территории, включая введение светофорного регулирования, расширение и обособление островка безопасности, перенос остновки и создание нового пешеходного перехода.\n",
    "В дополнение к этим недостаткам перекрёстка сейчас выяснилось и то, что улица шире, чем должна быть по нормативам, она идёт под уклон, знак о снижении скорости до 40 км/ч перед школой плохо виден из-за рекламных щитов и указателей, а сам перекрёсток, на котором сбили детей, не освещён.\n",
    "Светофор диктует правила. Кто нарушает — тот и виноват. Но с точки зрения программы «Ноль смертей», нужно не карать за нарушения, а предотвращать их. Дизайн улицы нужно продумать так, чтобы нарушать было просто некомфортно, а соблюдение правил стало делом совершенно естественным. Чем шире полосы, тем выше искушение вдавить педаль в пол. Особенно в комплекте с горочкой. Люди могут не отдавать себе в этом отчета — они просто едут с комфортной для них скоростью, которая выше установленных ограничений. А чем выше скорость, тем ӯже угол обзора и длиннее тормозной путь. Если на проезжей части происходит что-то неожиданное, велик риск, что водитель не успеет на это вовремя среагировать и затормозить. Хотя пара мальчишек на переходе возле школы едва ли может считаться неожиданностью.\n",
    "Источник: «Городские проекты».\n",
    "Следующий материал посвящён организационной стороне вопроса. Что нужно делать политикам, если они намереваются поддерживать программу Vision Zero и бороться со смертностью на дорогах? Всего лишь три пункта:\n",
    "1. План действий и понятные цели. Снижение скорости, редизайн улиц, информационная кампания — всё это должно быть проработано и предложено ещё до внедрения новой стратегии.\n",
    "2. Государственное финансирование. Гибель и травмы на дорогах — это не только личные трагедии, но и ущерб для экономики страны. И того и другого можно избежать с помощью изменения подхода к безопасности уже на этапе проектирования дорог.\n",
    "3. Готовность к сопротивлению автомобилистов. Горожане и СМИ будут критиковать политиков, призывающих к снижению скорости. Возможно, для преодоления общественного мнения сперва потребуется ввести «непопулярные» меры в социально значимых районах — у больниц и школ, постепенно распространяя их всё дальше и дальше.\n",
    "По данным одного исследования, 14 стран, в период с 1981 по 1999 год ставившие себе четкие задачи по снижению смертности в ДТП, действительно этого добились. Да, наша конечная цель — ноль смертей, но пока она не достигнута, надо стремиться к каким-то промежуточным результатам. Это позволяет оценивать прогресс и использовать релевантные инструменты.\n",
    "Источник: «Городские проекты».\n",
    "И последний текст — для журналистов. В канадском исследовании, посвящённом заметкам о ДТП, в которых погибали пешеходы и велосипедисты, перечислены ошибки их авторов.\n",
    "Из 71 материала лишь в трёх не употребляется страдательный залог. Остальные представляют из себя что-то вроде «Пешеход был сбит машиной». То есть водитель как бы и не при чём вовсе.\n",
    "Автомобиль стал виной ДТП в 33 случаях из 71. В остальных текстах водителю никоим образом не приписывается вина в содеянном. Лишь в двух статьях глагол «сбит» напрямую связан с водителем.\n",
    "Большая часть материалов вообще не описывает жертв ДТП. В 18 текстах указан возраст, в 8 — лишь эпитеты вроде «красивая девочка» или «всегда счастливая».\n",
    "Единственное исключение из этих правил — нетрезвые водители, вина которых, видимо, не вызывает у журналистов сомнения.\n",
    "Магусин подытоживает: «Смерть пешеходов преподносится как отдельно взятые несчастные случаи, вырванные из контекста подобных ДТП, которые происходят повсеместно. В них не учитывается человеческий фактор, а водители, благодаря используемому в новостных сообщениях языку, остаются на дистанции от случившегося». Далее она пишет: «С лингвистической и психологической точки зрения, это отражает сложившуюся реальность, в которой автомобили важнее пешеходов и их безопасности».\n",
    "Подробнее — в другом материале «Городских проектов», содержащем советы ВОЗ при освещении ДТП.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws4 = ['Автомобили', 'ДТП', 'Скорость']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = '''Лукашенко заявил, что не будет подписывать никакие документы по интеграции с Россией, если это будет противоречить принципам независимости и суверенитета Белоруссии. Кроме того, он не подпишет и отраслевые документы по интеграции, пока не будут решены спорные вопросы экономического характера - речь, понятно, идет о поставках углеводородов.\n",
    "Лукашенко понимает, что вопрос создания Союзного государства принципиально важен для Кремля, так как развязывает руки в вопросе трансферта - а проще говоря, открывает новые возможности для продолжения узурпации власти Путиным. Вариант с \"тандемом\" уже не вариант, так как увеличенный в полтора раза срок правления - шесть лет - делает даже один срок зиц-президента слишком длинным. Просто невыносимо длинным - так и окочуриться можно, ожидая, когда снова можно будет идти в президенты. Остается либо переформатирование всей \"вертикали\" с созданием неконституционных органов управления, в которые перетечет вся государственная власть (это потребует серьезного редактирования Конституции, хотя ручной Конституционный суд может помочь и в этом вопросе, узаконив любые махинации, как это уже произошло с третьим и четвертым сроками). Переформатирование \"вертикали\" в условиях идущего полным ходом развала управления выглядит весьма рискованным. Союзное государство - наименее болезненный выход из положения, так как можно решить все болезненные вопросы келейно, тихо и постепенно. При условии, конечно, что Путин станет фюрером нового государства, а Лукашенко тихо уйдет в сторонку. По возможности навсегда, так как двух каудильо при диктатуре быть не может по определению.\n",
    "Лукашенко прекрасно понимает суть момента, а потому совершенно не собирается играть в чужие игры, но при этом выторговывая максимум возможного до того, как окончательно скажет \"нет\". Собственно, его заявление - это уже \"нет\". Невозможно создать интеграционный проект, не ущемив национальные интересы, не передав часть суверенитета в этот проект. А потому клятва не поступиться суверенитетом равнозначна отказу от интеграции. Впрочем, и это может быть игрой, выторговыванием тех или иных уступок.\n",
    "Однако скорее всего, никакого Союзного государства не будет - только полный идиот будет бросаться в объятия к нынешнему Кремлю. Современная Россия - обычный империалистический хищник, точнее, что-то ниже классом - вроде шакала. Договариваться с ним о чем угодно может только прекраснодушный романтик. Лукашенко на него похож мало.\n",
    "Правда, складывающаяся ситуация создает и для самого Лукашенко серьезные проблемы - у него скоро выборы, и не идти на них - рисковать тем, что Кремль сумеет продвинуть какую-то свою креатуру и добить все-таки вопрос. А значит - Лукашенко снова идет на выборы. В который раз. Об этом уже объявлено - сегодня.\n",
    "Это тоже не айс, так как раздражение его бессменным руководством в Белоруссии имеется вполне нешуточное. Да и срок правления бесконечно затягивать просто опасно - для авторитарных диктатур передача власти - вопрос крайне болезненный, и затягивание этого вопроса с каждой новой итерацией создает все более прогрессирующие риски. Ну, и не стоит упоминать даже о том, что руководитель страны, засидевшийся у руля, в определенный момент начинает тратить все свои силы и ресурсы страны не на развитие, а просто на удержание власти. Что ведет страну к стагнации и кризису.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws5 = ['Белоруссия', 'Россия']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t1.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(t1)\n",
    "with open('t2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(t2)\n",
    "with open('t3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(t3)\n",
    "with open('t4.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(t4)\n",
    "with open('t5.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my words\n",
    "ws12 = ['отсутсвие', 'достоинство']\n",
    "ws22 = ['РСЗО', 'Россия', 'Украина']\n",
    "ws32 = ['логотип', 'Санкт-Петербург', 'студия', 'Лебедев']\n",
    "ws42 = ['смертность', 'машина', 'пешеход']\n",
    "ws52 = ['Лукашенко', 'суверенитет']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "m = MorphAnalyzer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    lemmas = []\n",
    "    for token in simple_word_tokenize(text.replace('-', ' ')):\n",
    "        lemmas.append(m.parse(token)[0].normal_form)\n",
    "    return ' '.join(lemmas).replace('рсзый', 'рсзо')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt1 = normalize_text(t1)\n",
    "nt2 = normalize_text(t2)\n",
    "nt3 = normalize_text(t3)\n",
    "nt4 = normalize_text(t4)\n",
    "nt5 = normalize_text(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "nws1 = []\n",
    "nws2 = []\n",
    "nws3 = []\n",
    "nws4 = []\n",
    "nws5 = []\n",
    "nws12 = []\n",
    "nws22 = []\n",
    "nws32 = []\n",
    "nws42 = []\n",
    "nws52 = []\n",
    "for w in ws1:\n",
    "    nws1.append(normalize(w))\n",
    "for w in ws2:\n",
    "    nws2.append(normalize(w))\n",
    "for w in ws3:\n",
    "    nws3.append(normalize(w))\n",
    "for w in ws4:\n",
    "    nws4.append(normalize(w))\n",
    "for w in ws5:\n",
    "    nws5.append(normalize(w))\n",
    "for w in ws12:\n",
    "    nws12.append(normalize(w))\n",
    "for w in ws22:\n",
    "    nws22.append(normalize(w))\n",
    "for w in ws32:\n",
    "    nws32.append(normalize(w))\n",
    "for w in ws42:\n",
    "    nws42.append(normalize(w))\n",
    "for w in ws52:\n",
    "    nws52.append(normalize(w))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/masha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('russian')\n",
    "stop.append('это')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/bin-osx/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/models/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< t1.txt > t1.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/bin-osx/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/models/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< t2.txt > t2.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/bin-osx/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/models/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< t3.txt > t3.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/bin-osx/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/models/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< t4.txt > t4.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/bin-osx/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "/Users/masha/Desktop/need_for_vyshka/P_for_Proga/python/udpipe-1.2.0-bin/models/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< t5.txt > t5.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees1 = []\n",
    "\n",
    "with open('t1.conllu') as f:\n",
    "    parsed_sents = f.read().split('\\n\\n')\n",
    "\n",
    "    for sent in parsed_sents:\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees1.append('\\n'.join(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees2 = []\n",
    "\n",
    "with open('t2.conllu') as f:\n",
    "    parsed_sents = f.read().split('\\n\\n')\n",
    "\n",
    "    for sent in parsed_sents:\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees2.append('\\n'.join(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees3 = []\n",
    "\n",
    "with open('t3.conllu') as f:\n",
    "    parsed_sents = f.read().split('\\n\\n')\n",
    "\n",
    "    for sent in parsed_sents:\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees3.append('\\n'.join(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees4 = []\n",
    "\n",
    "with open('t4.conllu') as f:\n",
    "    parsed_sents = f.read().split('\\n\\n')\n",
    "\n",
    "    for sent in parsed_sents:\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees4.append('\\n'.join(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees5 = []\n",
    "\n",
    "with open('t5.conllu') as f:\n",
    "    parsed_sents = f.read().split('\\n\\n')\n",
    "\n",
    "    for sent in parsed_sents:\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees5.append('\\n'.join(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FILTER_RELS = ['punct', 'conj', 'parataxis']\n",
    "def get_subtree(nodes, node):\n",
    "    if not nodes[node]['deps']:\n",
    "        return [node]\n",
    "    else:\n",
    "        return [node] + [get_subtree(nodes, dep) for rel in nodes[node]['deps'] \n",
    "                         if rel not in _FILTER_RELS\n",
    "                         for dep in nodes[node]['deps'][rel]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    flat = []\n",
    "    for el in l:\n",
    "        if not isinstance(el, list):\n",
    "            flat.append(el)\n",
    "        else:\n",
    "            flat += flatten(el)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = []\n",
    "for t in trees1:\n",
    "    g1 = DependencyGraph(t, top_relation_label='root')\n",
    "    np_list1 = []\n",
    "    for n in g1.nodes:\n",
    "        if g1.nodes[n]['ctag'] == 'NOUN':\n",
    "            np = list(sorted(flatten(get_subtree(g1.nodes, n))))\n",
    "            np_list1.append(' '.join([g1.nodes[i]['word'] for i in np]))\n",
    "    result1.append(np_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = []\n",
    "for t in trees2:\n",
    "    g2 = DependencyGraph(t, top_relation_label='root')\n",
    "    np_list2 = []\n",
    "    for n in g2.nodes:\n",
    "        if g2.nodes[n]['ctag'] == 'NOUN':\n",
    "            np = list(sorted(flatten(get_subtree(g2.nodes, n))))\n",
    "            np_list2.append(' '.join([g2.nodes[i]['word'] for i in np]))\n",
    "    result2.append(np_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = []\n",
    "for t in trees3:\n",
    "    g3 = DependencyGraph(t, top_relation_label='root')\n",
    "    np_list3 = []\n",
    "    for n in g3.nodes:\n",
    "        if g3.nodes[n]['ctag'] == 'NOUN':\n",
    "            np = list(sorted(flatten(get_subtree(g3.nodes, n))))\n",
    "            np_list3.append(' '.join([g3.nodes[i]['word'] for i in np]))\n",
    "    result3.append(np_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = []\n",
    "for t in trees4:\n",
    "    g4 = DependencyGraph(t, top_relation_label='root')\n",
    "    np_list4 = []\n",
    "    for n in g4.nodes:\n",
    "        if g4.nodes[n]['ctag'] == 'NOUN':\n",
    "            np = list(sorted(flatten(get_subtree(g4.nodes, n))))\n",
    "            np_list4.append(' '.join([g4.nodes[i]['word'] for i in np]))\n",
    "    result4.append(np_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = []\n",
    "for t in trees5:\n",
    "    g5 = DependencyGraph(t, top_relation_label='root')\n",
    "    np_list5 = []\n",
    "    for n in g5.nodes:\n",
    "        if g5.nodes[n]['ctag'] == 'NOUN':\n",
    "            np = list(sorted(flatten(get_subtree(g5.nodes, n))))\n",
    "            np_list5.append(' '.join([g5.nodes[i]['word'] for i in np]))\n",
    "    result5.append(np_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "rake = RAKE.Rake(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake1 = rake.run(nt1, maxWords=1)\n",
    "rake2 = rake.run(nt2, maxWords=1)\n",
    "rake3 = rake.run(nt3, maxWords=1)\n",
    "rake4 = rake.run(nt4, maxWords=1)\n",
    "rake5 = rake.run(nt5, maxWords=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ок'}"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in rake1]) & set(flatten(result1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in rake2]) & set(flatten(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'спасибо'}"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in rake3]) & set(flatten(result3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'автомобилизация',\n",
       " 'автомобилист',\n",
       " 'водитель',\n",
       " 'знак',\n",
       " 'расширение',\n",
       " 'скорость',\n",
       " 'смертность',\n",
       " 'человек',\n",
       " 'школа'}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in rake4]) & set(flatten(result4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'вопрос'}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in rake5]) & set(flatten(result5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('оксана', 2.0),\n",
       " ('ок', 1.3333333333333333),\n",
       " ('достоинство', 1.3333333333333333),\n",
       " ('глаз', 1.0),\n",
       " ('взгляд', 1.0),\n",
       " ('пусть', 1.0)]"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake.run(normalize_text(t1), minFrequency=2, maxWords=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('обычный выходной', 4.0),\n",
       " ('торнадо год', 4.0),\n",
       " ('рсзо', 1.8),\n",
       " ('россия', 1.0),\n",
       " ('град', 1.0)]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake.run(normalize_text(t2), minFrequency=2, maxWords=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('отличный логотип', 4.4), ('который', 1.0), ('—', 0)]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake.run(normalize_text(t3), minFrequency=2, maxWords=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('нью йорк', 4.333333333333334),\n",
       " ('снижение скорость', 4.119047619047619),\n",
       " ('который', 2.142857142857143),\n",
       " ('дтп', 2.142857142857143),\n",
       " ('смертность', 2.1),\n",
       " ('пешеход', 1.7),\n",
       " ('должный', 1.6666666666666667),\n",
       " ('травма', 1.6666666666666667),\n",
       " ('миллиард', 1.6666666666666667),\n",
       " ('дорога', 1.5555555555555556),\n",
       " ('безопасность', 1.5),\n",
       " ('лишь', 1.5),\n",
       " ('2017 год', 1.4545454545454546),\n",
       " ('гибель', 1.3333333333333333),\n",
       " ('результат', 1.3333333333333333),\n",
       " ('источник', 1.0),\n",
       " ('половина', 1.0),\n",
       " ('далеко', 1.0),\n",
       " ('вместе', 1.0),\n",
       " ('знать', 1.0),\n",
       " ('разрыв', 1.0),\n",
       " ('оставаться', 1.0),\n",
       " ('водитель', 1.0),\n",
       " ('«', 0)]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake.run(normalize_text(t4), minFrequency=2, maxWords=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('вопрос', 1.3333333333333333),\n",
       " ('мочь', 1.3333333333333333),\n",
       " ('интеграция', 1.0),\n",
       " ('идти', 1.0),\n",
       " ('вариант', 1.0),\n",
       " ('вертикаль', 1.0)]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake.run(normalize_text(t5), minFrequency=2, maxWords=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf, bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1206 + 718 + 1112 + 11610 + 3293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1163, 654, 1016, 11146, 3217]\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "ls.append(len(nt1))\n",
    "ls.append(len(nt2))\n",
    "ls.append(len(nt3))\n",
    "ls.append(len(nt4))\n",
    "ls.append(len(nt5))\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3439.2\n"
     ]
    }
   ],
   "source": [
    "avgdl = 0\n",
    "for l in ls:\n",
    "    avgdl += l\n",
    "avgdl = avgdl/len(ls)\n",
    "print(avgdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    lemmas = [m.parse(t)[0].normal_form for t in tokens]\n",
    "    nphrase = ' '.join(lemmas)\n",
    "    nphrase = nphrase.replace('рсзый', 'рсзо')\n",
    "    nphrase = nphrase.replace('«', '')\n",
    "    nphrase = nphrase.replace('»', '')\n",
    "    nphrase = nphrase.replace('\"', '')\n",
    "    nphrase = nphrase.replace(',', '')\n",
    "    nphrase = nphrase.replace('.', '')\n",
    "    nphrase = nphrase.replace('!', '')\n",
    "    nphrase = nphrase.replace('?', '')\n",
    "    nphrase = nphrase.replace('\\n', ' ')\n",
    "    nphrase = nphrase.replace('—', '')\n",
    "    nphrase = nphrase.replace('-', ' ')\n",
    "    nphrase = nphrase.replace(':', '')\n",
    "    nphrase = nphrase.replace('  ', ' ')\n",
    "    return nphrase\n",
    "nt1 = normalize(t1)\n",
    "nt2 = normalize(t2)\n",
    "nt3 = normalize(t3)\n",
    "nt4 = normalize(t4)\n",
    "nt5 = normalize(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for word in nt1.split(' '):\n",
    "    if word not in words:\n",
    "        words.append(word)\n",
    "for word in nt2.split(' '):\n",
    "    if word not in words:\n",
    "        words.append(word)\n",
    "for word in nt3.split(' '):\n",
    "    if word not in words:\n",
    "        words.append(word)\n",
    "for word in nt4.split(' '):\n",
    "    if word not in words:\n",
    "        words.append(word)\n",
    "for word in nt5.split(' '):\n",
    "    if word not in words:\n",
    "        words.append(word)\n",
    "lw = len(words)\n",
    "print(lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = {}\n",
    "for g in range(lw):\n",
    "    dw[words[g]] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = []\n",
    "for q in range(N):\n",
    "    vec.append(0)\n",
    "TF = []\n",
    "TFIDF = []\n",
    "BM25 = []\n",
    "for w in range(lw):\n",
    "    TF.append(copy.deepcopy(vec))\n",
    "    TFIDF.append(copy.deepcopy(vec))\n",
    "    BM25.append(copy.deepcopy(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq = []\n",
    "for w in range(lw):\n",
    "    nq.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = []\n",
    "for w in range(lw):\n",
    "    IDF.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmock = []\n",
    "for word in nt1.split(' '):\n",
    "    w = dw[word]\n",
    "    TF[w][0] = ((TF[w][0] * ls[0]) + 1) / ls[0]\n",
    "    if word in qmock:\n",
    "        nq[w] += 1\n",
    "    qmock.append(word)\n",
    "for word in nt2.split(' '):\n",
    "    w = dw[word]\n",
    "    TF[w][1] = ((TF[w][1] * ls[1]) + 1) / ls[1]\n",
    "    if word in qmock:\n",
    "        nq[w] += 1\n",
    "    qmock.append(word)\n",
    "for word in nt3.split(' '):\n",
    "    w = dw[word]\n",
    "    TF[w][2] = ((TF[w][2] * ls[2]) + 1) / ls[2]\n",
    "    if word in qmock:\n",
    "        nq[w] += 1\n",
    "    qmock.append(word)\n",
    "for word in nt4.split(' '):\n",
    "    w = dw[word]\n",
    "    TF[w][3] = ((TF[w][3] * ls[3]) + 1) / ls[3]\n",
    "    if word in qmock:\n",
    "        nq[w] += 1\n",
    "    qmock.append(word)\n",
    "for word in nt5.split(' '):\n",
    "    w = dw[word]\n",
    "    TF[w][4] = ((TF[w][4] * ls[4]) + 1) / ls[4]\n",
    "    if word in qmock:\n",
    "        nq[w] += 1\n",
    "    qmock.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2.0\n",
    "b = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nt1.split(' '):\n",
    "    pw = dw[word]\n",
    "    IDF[pw] = math.log((N - nq[pw] + 0.5)/(nq[pw] + 0.5))\n",
    "    TFIDF[pw][0] = IDF[pw]*TF[pw][0]\n",
    "    BM25[pw][0] = IDF[pw] * ((TF[pw][0] * (k + 1))/(TF[pw][0] + k*(1 - b + b*ls[0]/avgdl)))\n",
    "for word in nt2.split(' '):\n",
    "    pw = dw[word]\n",
    "    IDF[pw] = math.log((N - nq[pw] + 0.5)/(nq[pw] + 0.5))\n",
    "    TFIDF[pw][1] = IDF[pw]*TF[pw][1]\n",
    "    BM25[pw][1] = IDF[pw] * ((TF[pw][1] * (k + 1))/(TF[pw][1] + k*(1 - b + b*ls[1]/avgdl)))\n",
    "for word in nt3.split(' '):\n",
    "    pw = dw[word]\n",
    "    IDF[pw] = math.log((N - nq[pw] + 0.5)/(nq[pw] + 0.5))\n",
    "    TFIDF[pw][2] = IDF[pw]*TF[pw][2]\n",
    "    BM25[pw][2] = IDF[pw] * ((TF[pw][2] * (k + 1))/(TF[pw][2] + k*(1 - b + b*ls[2]/avgdl)))\n",
    "for word in nt4.split(' '):\n",
    "    pw = dw[word]\n",
    "    IDF[pw] = math.log((N - nq[pw] + 0.5)/(nq[pw] + 0.5))\n",
    "    TFIDF[pw][3] = IDF[pw]*TF[pw][3]\n",
    "    BM25[pw][3] = IDF[pw] * ((TF[pw][3] * (k + 1))/(TF[pw][3] + k*(1 - b + b*ls[3]/avgdl)))\n",
    "for word in nt5.split(' '):\n",
    "    pw = dw[word]\n",
    "    IDF[pw] = math.log((N - nq[pw] + 0.5)/(nq[pw] + 0.5))\n",
    "    TFIDF[pw][4] = IDF[pw]*TF[pw][4]\n",
    "    BM25[pw][4] = IDF[pw] * ((TF[pw][4] * (k + 1))/(TF[pw][4] + k*(1 - b + b*ls[4]/avgdl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1tf = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt1.split(' '):\n",
    "    if word not in top1tf and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if TFIDF[pw][0] > score1:\n",
    "            top1tf[3] = top1tf[2]\n",
    "            top1tf[2] = top1tf[1]\n",
    "            top1tf[1] = top1tf[0]\n",
    "            top1tf[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = TFIDF[pw][0]\n",
    "        elif TFIDF[pw][0] > score2:\n",
    "            top1tf[3] = top1tf[2]\n",
    "            top1tf[2] = top1tf[1]\n",
    "            top1tf[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = TFIDF[pw][0]\n",
    "        elif TFIDF[pw][0] > score3:\n",
    "            top1tf[3] = top1tf[2]\n",
    "            top1tf[2] = word\n",
    "            score4 = score3\n",
    "            score3 = TFIDF[pw][0]\n",
    "        elif TFIDF[pw][0] > score4:\n",
    "            top1tf[3] = word\n",
    "            score4 = TFIDF[pw][0]\n",
    "        \n",
    "top2tf = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt2.split(' '):\n",
    "    if word not in top2tf and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if TFIDF[pw][1] > score1:\n",
    "            top2tf[3] = top2tf[2]\n",
    "            top2tf[2] = top2tf[1]\n",
    "            top2tf[1] = top2tf[0]\n",
    "            top2tf[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = TFIDF[pw][1]\n",
    "        elif TFIDF[pw][1] > score2:\n",
    "            top2tf[3] = top2tf[2]\n",
    "            top2tf[2] = top2tf[1]\n",
    "            top2tf[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = TFIDF[pw][1]\n",
    "        elif TFIDF[pw][0] > score3:\n",
    "            top2tf[3] = top2tf[2]\n",
    "            top2tf[2] = word\n",
    "            score4 = score3\n",
    "            score3 = TFIDF[pw][1]\n",
    "        elif TFIDF[pw][1] > score4:\n",
    "            top2tf[3] = word\n",
    "            score4 = TFIDF[pw][1]\n",
    "\n",
    "top3tf = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt3.split(' '):\n",
    "    if word not in top3tf and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if TFIDF[pw][2] > score1:\n",
    "            top3tf[3] = top3tf[2]\n",
    "            top3tf[2] = top3tf[1]\n",
    "            top3tf[1] = top3tf[0]\n",
    "            top3tf[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = TFIDF[pw][2]\n",
    "        elif TFIDF[pw][2] > score2:\n",
    "            top3tf[3] = top3tf[2]\n",
    "            top3tf[2] = top3tf[1]\n",
    "            top3tf[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = TFIDF[pw][2]\n",
    "        elif TFIDF[pw][2] > score3:\n",
    "            top3tf[3] = top3tf[2]\n",
    "            top3tf[2] = word\n",
    "            score4 = score3\n",
    "            score3 = TFIDF[pw][2]\n",
    "        elif TFIDF[pw][2] > score4:\n",
    "            top3tf[3] = word\n",
    "            score4 = TFIDF[pw][2]\n",
    "        \n",
    "top4tf = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt4.split(' '):\n",
    "    if word not in top4tf and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if TFIDF[pw][3] > score1:\n",
    "            top4tf[3] = top4tf[2]\n",
    "            top4tf[2] = top4tf[1]\n",
    "            top4tf[1] = top4tf[0]\n",
    "            top4tf[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = TFIDF[pw][3]\n",
    "        elif TFIDF[pw][3] > score2:\n",
    "            top4tf[3] = top4tf[2]\n",
    "            top4tf[2] = top4tf[1]\n",
    "            top4tf[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = TFIDF[pw][3]\n",
    "        elif TFIDF[pw][3] > score3:\n",
    "            top4tf[3] = top4tf[2]\n",
    "            top4tf[2] = word\n",
    "            score4 = score3\n",
    "            score3 = TFIDF[pw][3]\n",
    "        elif TFIDF[pw][3] > score4:\n",
    "            top4tf[3] = word\n",
    "            score4 = TFIDF[pw][3]\n",
    "        \n",
    "top5tf = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt5.split(' '):\n",
    "    if word not in top5tf and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if TFIDF[pw][4] > score1:\n",
    "            top5tf[3] = top5tf[2]\n",
    "            top5tf[2] = top5tf[1]\n",
    "            top5tf[1] = top5tf[0]\n",
    "            top5tf[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = TFIDF[pw][4]\n",
    "        elif TFIDF[pw][4] > score2:\n",
    "            top5tf[3] = top5tf[2]\n",
    "            top5tf[2] = top5tf[1]\n",
    "            top5tf[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = TFIDF[pw][4]\n",
    "        elif TFIDF[pw][4] > score3:\n",
    "            top5tf[3] = top5tf[2]\n",
    "            top5tf[2] = word\n",
    "            score4 = score3\n",
    "            score3 = TFIDF[pw][4]\n",
    "        elif TFIDF[pw][4] > score4:\n",
    "            top5tf[3] = word\n",
    "            score4 = TFIDF[pw][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1bm = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt1.split(' '):\n",
    "    if word not in top1bm and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if BM25[pw][0] > score1:\n",
    "            top1bm[3] = top1bm[2]\n",
    "            top1bm[2] = top1bm[1]\n",
    "            top1bm[1] = top1bm[0]\n",
    "            top1bm[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = BM25[pw][0]\n",
    "        elif BM25[pw][0] > score2:\n",
    "            top1bm[3] = top1bm[2]\n",
    "            top1bm[2] = top1bm[1]\n",
    "            top1bm[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = BM25[pw][0]\n",
    "        elif BM25[pw][0] > score3:\n",
    "            top1bm[3] = top1bm[2]\n",
    "            top1bm[2] = word\n",
    "            score4 = score3\n",
    "            score3 = BM25[pw][0]\n",
    "        elif BM25[pw][0] > score4:\n",
    "            top1bm[3] = word\n",
    "            score4 = BM25[pw][0]\n",
    "        \n",
    "top2bm = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt2.split(' '):\n",
    "    if word not in top2bm and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if BM25[pw][1] > score1:\n",
    "            top2bm[3] = top2bm[2]\n",
    "            top2bm[2] = top2bm[1]\n",
    "            top2bm[1] = top2bm[0]\n",
    "            top2bm[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = BM25[pw][1]\n",
    "        elif BM25[pw][1] > score2:\n",
    "            top2bm[3] = top2bm[2]\n",
    "            top2bm[2] = top2bm[1]\n",
    "            top2bm[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = BM25[pw][1]\n",
    "        elif BM25[pw][0] > score3:\n",
    "            top2bm[3] = top2bm[2]\n",
    "            top2bm[2] = word\n",
    "            score4 = score3\n",
    "            score3 = BM25[pw][1]\n",
    "        elif BM25[pw][1] > score4:\n",
    "            top2bm[3] = word\n",
    "            score4 = BM25[pw][1]\n",
    "\n",
    "top3bm = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt3.split(' '):\n",
    "    if word not in top3bm and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if BM25[pw][2] > score1:\n",
    "            top3bm[3] = top3bm[2]\n",
    "            top3bm[2] = top3bm[1]\n",
    "            top3bm[1] = top3bm[0]\n",
    "            top3bm[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = BM25[pw][2]\n",
    "        elif BM25[pw][2] > score2:\n",
    "            top3bm[3] = top3bm[2]\n",
    "            top3bm[2] = top3bm[1]\n",
    "            top3bm[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = BM25[pw][2]\n",
    "        elif BM25[pw][2] > score3:\n",
    "            top3bm[3] = top3bm[2]\n",
    "            top3bm[2] = word\n",
    "            score4 = score3\n",
    "            score3 = BM25[pw][2]\n",
    "        elif BM25[pw][2] > score4:\n",
    "            top3bm[3] = word\n",
    "            score4 = BM25[pw][2]\n",
    "        \n",
    "top4bm = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt4.split(' '):\n",
    "    if word not in top4bm and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if BM25[pw][3] > score1:\n",
    "            top4bm[3] = top4bm[2]\n",
    "            top4bm[2] = top4bm[1]\n",
    "            top4bm[1] = top4bm[0]\n",
    "            top4bm[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = BM25[pw][3]\n",
    "        elif BM25[pw][3] > score2:\n",
    "            top4bm[3] = top4bm[2]\n",
    "            top4bm[2] = top4bm[1]\n",
    "            top4bm[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = BM25[pw][3]\n",
    "        elif BM25[pw][3] > score3:\n",
    "            top4bm[3] = top4bm[2]\n",
    "            top4bm[2] = word\n",
    "            score4 = score3\n",
    "            score3 = BM25[pw][3]\n",
    "        elif BM25[pw][3] > score4:\n",
    "            top4bm[3] = word\n",
    "            score4 = BM25[pw][3]\n",
    "        \n",
    "top5bm = ['', '', '', '']\n",
    "score1 = 0\n",
    "score2 = 0\n",
    "score3 = 0\n",
    "score4 = 0\n",
    "for word in nt5.split(' '):\n",
    "    if word not in top5bm and word not in stop:\n",
    "        pw = dw[word]\n",
    "        if BM25[pw][4] > score1:\n",
    "            top5bm[3] = top5bm[2]\n",
    "            top5bm[2] = top5bm[1]\n",
    "            top5bm[1] = top5bm[0]\n",
    "            top5bm[0] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = score1\n",
    "            score1 = BM25[pw][4]\n",
    "        elif BM25[pw][4] > score2:\n",
    "            top5bm[3] = top5bm[2]\n",
    "            top5bm[2] = top5bm[1]\n",
    "            top5bm[1] = word\n",
    "            score4 = score3\n",
    "            score3 = score2\n",
    "            score2 = BM25[pw][4]\n",
    "        elif BM25[pw][4] > score3:\n",
    "            top5bm[3] = top5bm[2]\n",
    "            top5bm[2] = word\n",
    "            score4 = score3\n",
    "            score3 = BM25[pw][4]\n",
    "        elif BM25[pw][4] > score4:\n",
    "            top5bm[3] = word\n",
    "            score4 = BM25[pw][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'ок', 'оксана', 'сбоку']\n",
      "['рсзо', 'выходной', '', 'ракет']\n",
      "['логотип', 'новый', 'студия', 'лебедева']\n",
      "['дорогах', 'городские', 'проекты', 'дтп']\n",
      "['', 'лукашенко', 'интеграции', 'государства']\n",
      "['', 'ок', 'оксана', 'сбоку']\n",
      "['рсзо', 'выходной', 'обычный', 'торнадо']\n",
      "['логотип', 'новый', 'студия', 'лебедева']\n",
      "['дорогах', 'городские', 'проекты', 'дтп']\n",
      "['', 'лукашенко', 'интеграции', 'государства']\n"
     ]
    }
   ],
   "source": [
    "print(top1tf)\n",
    "print(top2tf)\n",
    "print(top3tf) \n",
    "print(top4tf)     \n",
    "print(top5tf)\n",
    "print(top1bm)\n",
    "print(top2bm)\n",
    "print(top3bm) \n",
    "print(top4bm)     \n",
    "print(top5bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
